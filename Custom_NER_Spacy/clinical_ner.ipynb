{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bert/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE JSON\n",
      "TESTING ALGORITHM\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "from spacy.util import filter_spans\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training.example import Example\n",
    "from spacy.scorer import Scorer\n",
    "# Custom Written Functions to prepare dataset for NER Training\n",
    "from data_convert import create_dict_concept_type, find_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Training Data from BETH to JSON Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = glob.glob(\"datasets/concept_assertion_relation_training_data/beth/txt/*\")\n",
    "concept_files_path = \"datasets/concept_assertion_relation_training_data/beth/concept/\"\n",
    "training_data = {'classes' : ['TEST', \"TREATMENT\", \"PROBLEM\"], 'annotations' : []}\n",
    "\n",
    "\n",
    "for filename in text_files:\n",
    "    temp_dict = {}\n",
    "    with open(filename, \"r\") as file:\n",
    "        text = file.read()\n",
    "    temp_dict['entities'] = []\n",
    "    temp_dict['text'] = text\n",
    "    temp_dict[\"file_name\"] = filename.split(\"/\")[-1]\n",
    "\n",
    "    concept_filename = filename.split(\"/\")[-1].replace(\"txt\",\"con\")\n",
    "    concept_path = concept_files_path + concept_filename\n",
    "    dict_concept_type = create_dict_concept_type(concept_path)\n",
    "    \n",
    "    for concept, _type in dict_concept_type.items():\n",
    "        length_word = len(concept)+1\n",
    "        indexes = find_index(text,concept)\n",
    "   \n",
    "        for index in indexes:\n",
    "            temp_dict[\"entities\"].append((index[0],index[1]+1,_type.upper()))\n",
    "    training_data[\"annotations\"].append(temp_dict)\n",
    "    \n",
    "if not os.path.exists('datasets/json_files/beth_data.json'):\n",
    "    with open(\"datasets/json_files/beth_data.json\", \"w\") as file:\n",
    "        json.dump(training_data, file, indent=4)\n",
    "        print(\"DONE JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Training Data from Partners to JSON Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = glob.glob(\"datasets/concept_assertion_relation_training_data/partners/txt/*\")\n",
    "concept_files_path = \"datasets/concept_assertion_relation_training_data/partners/concept/\"\n",
    "training_data = {'classes' : ['TEST', \"TREATMENT\", \"PROBLEM\"], 'annotations' : []}\n",
    "\n",
    "\n",
    "for filename in text_files:\n",
    "    temp_dict = {}\n",
    "    with open(filename, \"r\") as file:\n",
    "        text = file.read()\n",
    "    temp_dict['entities'] = []\n",
    "    temp_dict['text'] = text\n",
    "    temp_dict[\"file_name\"] = filename.split(\"/\")[-1]\n",
    "\n",
    "    concept_filename = filename.split(\"/\")[-1].replace(\"txt\",\"con\")\n",
    "    concept_path = concept_files_path + concept_filename\n",
    "    dict_concept_type = create_dict_concept_type(concept_path)\n",
    "    \n",
    "    for concept, _type in dict_concept_type.items():\n",
    "        length_word = len(concept)+1\n",
    "        indexes = find_index(text,concept)\n",
    "   \n",
    "        for index in indexes:\n",
    "            temp_dict[\"entities\"].append((index[0],index[1]+1,_type.upper()))\n",
    "    training_data[\"annotations\"].append(temp_dict)\n",
    "    \n",
    "if not os.path.exists('datasets/json_files/partners_data.json'):\n",
    "    with open(\"datasets/json_files/partners_data.json\", \"w\") as file:\n",
    "        json.dump(training_data, file, indent=4)\n",
    "        print(\"DONE JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Test Data to JSON Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = glob.glob('datasets/reference_standard_for_test_data/txt/*')\n",
    "concept_files_path = \"datasets/reference_standard_for_test_data/concepts/\"\n",
    "test_data = {'classes' : ['TEST', \"TREATMENT\", \"PROBLEM\"], 'annotations' : []}\n",
    "\n",
    "\n",
    "for filename in text_files:\n",
    "    temp_dict = {}\n",
    "    with open(filename, \"r\") as file:\n",
    "        text = file.read()\n",
    "    temp_dict['entities'] = []\n",
    "    temp_dict['text'] = text\n",
    "    temp_dict[\"file_name\"] = filename.split(\"/\")[-1]\n",
    "\n",
    "    concept_filename = filename.split(\"/\")[-1].replace(\"txt\",\"con\")\n",
    "    concept_path = concept_files_path + concept_filename\n",
    "    dict_concept_type = create_dict_concept_type(concept_path)\n",
    "    \n",
    "    for concept, _type in dict_concept_type.items():\n",
    "        length_word = len(concept)+1\n",
    "        indexes = find_index(text,concept)\n",
    "   \n",
    "        for index in indexes:\n",
    "            temp_dict[\"entities\"].append((index[0],index[1]+1,_type.upper()))\n",
    "    test_data[\"annotations\"].append(temp_dict)\n",
    "    \n",
    "if not os.path.exists('datasets/json_files/test_data.json'):\n",
    "    with open(\"datasets/json_files/test_data.json\", \"w\") as file:\n",
    "        json.dump(test_data, file, indent=4)\n",
    "        print(\"DONE JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train dataset from the json file\n",
    "with open('datasets/json_files/beth_data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training Data in the form of a dictionary from json file\n",
    "training_data = {'classes': [\"TEST\", \"TREATMENT\", \"PROBLEM\"], 'annotations': []}\n",
    "for ann in data['annotations']:\n",
    "    temp_dict = {}\n",
    "    temp_dict['text'] = ann['text']\n",
    "    temp_dict['entities'] = []\n",
    "    for entity in ann['entities']:\n",
    "        temp_dict['entities'].append((entity[0],entity[1],entity[2]))\n",
    "    training_data['annotations'].append(temp_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 11/73 [00:00<00:01, 48.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 59/73 [00:00<00:00, 60.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:01<00:00, 59.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Code to Convert Training Data to Custom Spacy Format Required for Training\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin()\n",
    "for training_example in tqdm(training_data['annotations']):\n",
    "    text = training_example['text']\n",
    "    labels = training_example['entities']\n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents \n",
    "    doc_bin.add(doc)\n",
    "\n",
    "#Already created don't run again\n",
    "if not os.path.exists('training_data.spacy'):\n",
    "    doc_bin.to_disk(\"training_data.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please Don't Run Below Cell as it is already trained and it takes time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create Final Configuration File for Training from Base Configuration File\n",
    "if not os.path.exists('config.cfg'):\n",
    "    ! python -m spacy init fill-config base_config.cfg config.cfg\n",
    "\n",
    "# Run Custom NER Training using Spacy\n",
    "if not os.path.exists('model-best'):\n",
    "    ! python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train dataset from the json file\n",
    "with open('datasets/json_files/test_data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Data in the form of a dictionary from json file\n",
    "test_data = {'classes': [\"TEST\", \"TREATMENT\", \"PROBLEM\"], 'annotations': []}\n",
    "for ann in data['annotations']:\n",
    "    temp_dict = {}\n",
    "    temp_dict['text'] = ann['text']\n",
    "    temp_dict['entities'] = []\n",
    "    for entity in ann['entities']:\n",
    "        temp_dict['entities'].append((entity[0],entity[1],entity[2]))\n",
    "    test_data['annotations'].append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 67/256 [00:01<00:02, 77.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 85/256 [00:01<00:02, 79.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 113/256 [00:01<00:01, 84.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 150/256 [00:02<00:01, 80.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 180/256 [00:02<00:00, 82.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 209/256 [00:02<00:00, 85.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 248/256 [00:03<00:00, 90.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:03<00:00, 75.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Code to Convert Testing Data to Custom Spacy Format Required for Evaluation\n",
    "for testing_example in tqdm(test_data['annotations']):\n",
    "    text = testing_example['text']\n",
    "    labels = testing_example['entities']\n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents \n",
    "    doc_bin.add(doc)\n",
    "\n",
    "#Already created don't run again\n",
    "if not os.path.exists('testing_data.spacy'):\n",
    "    doc_bin.to_disk(\"testing_data.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "/opt/conda/envs/bert/lib/python3.7/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   83.33 \n",
      "NER R   76.93 \n",
      "NER F   80.00 \n",
      "SPEED   34280 \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "                P       R       F\n",
      "PROBLEM     80.83   77.50   79.13\n",
      "TEST        86.85   77.50   81.91\n",
      "TREATMENT   83.33   75.57   79.26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the Test Dataset\n",
    "! python -m spacy evaluate model-best/ ./testing_data.spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bert/lib/python3.7/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Full term well appearing infant with \n",
       "<mark class=\"entity\" style=\"background: #1f77b4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    some facial jaundice\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PROBLEM</span>\n",
       "</mark>\n",
       " , overall pink , warm and well perfused , alert and responsive .</br>Anterior fontanelle is soft , open and flat .</br>Ears are normally set .</br>\n",
       "<mark class=\"entity\" style=\"background: #2ca02c; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Red reflexes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TEST</span>\n",
       "</mark>\n",
       " noted bilaterally .</br>He had a left pupil that was in \n",
       "<mark class=\"entity\" style=\"background: #1f77b4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    unequal in size\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PROBLEM</span>\n",
       "</mark>\n",
       " with \n",
       "<mark class=\"entity\" style=\"background: #1f77b4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    a keyhole appearance\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PROBLEM</span>\n",
       "</mark>\n",
       " consistent with \n",
       "<mark class=\"entity\" style=\"background: #1f77b4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    a coloboma\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PROBLEM</span>\n",
       "</mark>\n",
       " .</br>The nares were patent .</br>Palate was intact .</br>Mucous membranes were moist and pink .</br>His neck was supple without \n",
       "<mark class=\"entity\" style=\"background: #1f77b4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    masses\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PROBLEM</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #1f77b4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    bruits\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PROBLEM</span>\n",
       "</mark>\n",
       " .</br>Lungs were clear to \n",
       "<mark class=\"entity\" style=\"background: #2ca02c; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    auscultation\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TEST</span>\n",
       "</mark>\n",
       " and equal .</br>Comfortable respiratory pattern .</br>Cardiovascular :</br>Regular rate and rhythm , no \n",
       "<mark class=\"entity\" style=\"background: #1f77b4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    murmur\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PROBLEM</span>\n",
       "</mark>\n",
       " .</br>2 plus \n",
       "<mark class=\"entity\" style=\"background: #2ca02c; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    femoral pulses\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TEST</span>\n",
       "</mark>\n",
       " were noted .</br>Abdomens oft with positive bowel sounds .</br>Genitourinary :</br>Infant was \n",
       "<mark class=\"entity\" style=\"background: #ff7f0e; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    circumcised\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TREATMENT</span>\n",
       "</mark>\n",
       " in the newborn nursery which was healing well .</br>Testes were descended bilaterally .</br>Extremities were pink and well perfused .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DISPLAY_COLORS = {\n",
    "    \"PROBLEM\": \"#1f77b4\",\n",
    "    \"TREATMENT\": \"#ff7f0e\",\n",
    "    \"TEST\": \"#2ca02c\",\n",
    "\n",
    "}\n",
    "nlp_ner = spacy.load('model-best')\n",
    "\n",
    "doc = nlp_ner('''Full term well appearing infant with some facial jaundice , overall pink , warm and well perfused , alert and responsive .\n",
    "Anterior fontanelle is soft , open and flat .\n",
    "Ears are normally set .\n",
    "Red reflexes noted bilaterally .\n",
    "He had a left pupil that was in unequal in size with a keyhole appearance consistent with a coloboma .\n",
    "The nares were patent .\n",
    "Palate was intact .\n",
    "Mucous membranes were moist and pink .\n",
    "His neck was supple without masses or bruits .\n",
    "Lungs were clear to auscultation and equal .\n",
    "Comfortable respiratory pattern .\n",
    "Cardiovascular :\n",
    "Regular rate and rhythm , no murmur .\n",
    "2 plus femoral pulses were noted .\n",
    "Abdomens oft with positive bowel sounds .\n",
    "Genitourinary :\n",
    "Infant was circumcised in the newborn nursery which was healing well .\n",
    "Testes were descended bilaterally .\n",
    "Extremities were pink and well perfused .''')\n",
    "\n",
    "#print(type(doc.ents[7]))\n",
    "\n",
    "options = {\"colors\": DISPLAY_COLORS} \n",
    "\n",
    "# Visulation of Entitites from Clinical Text\n",
    "spacy.displacy.render(doc, style=\"ent\", options= options, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scispacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_125888/1366324428.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscispacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscispacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scispacy'"
     ]
    }
   ],
   "source": [
    "import scispacy\n",
    "\n",
    "from scispacy.linking import EntityLinker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Entities and Similar Terms (Entity Linking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, ner, beam_ner, entity_ruler, tagger, morphologizer, senter, sentencizer, textcat, spancat, future_entity_ruler, span_ruler, textcat_multilabel, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_125888/1631992573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model-best\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scispacy_linker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"resolve_abbreviations\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"linker_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"umls\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m doc = nlp('''Full term well appearing infant with some facial jaundice , overall pink , warm and well perfused , alert and responsive .\n",
      "\u001b[0;32m/opt/conda/envs/bert/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    798\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0mraw_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                 \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m             )\n\u001b[1;32m    802\u001b[0m         \u001b[0mpipe_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pipe_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/bert/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_pipe\u001b[0;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    653\u001b[0m                 \u001b[0mlang_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             )\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0mpipe_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_factory_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# This is unideal, but the alternative would mean you always need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, ner, beam_ner, entity_ruler, tagger, morphologizer, senter, sentencizer, textcat, spancat, future_entity_ruler, span_ruler, textcat_multilabel, en.lemmatizer"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"model-best\")\n",
    "\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "\n",
    "doc = nlp('''Full term well appearing infant with some facial jaundice , overall pink , warm and well perfused , alert and responsive .\n",
    "Anterior fontanelle is soft , open and flat .\n",
    "Ears are normally set .\n",
    "Red reflexes noted bilaterally .\n",
    "He had a left pupil that was in unequal in size with a keyhole appearance consistent with a coloboma .\n",
    "The nares were patent .\n",
    "Palate was intact .\n",
    "Mucous membranes were moist and pink .\n",
    "His neck was supple without masses or bruits .\n",
    "Lungs were clear to auscultation and equal .\n",
    "Comfortable respiratory pattern .\n",
    "Cardiovascular :\n",
    "Regular rate and rhythm , no murmur .\n",
    "2 plus femoral pulses were noted .\n",
    "Abdomens oft with positive bowel sounds .\n",
    "Genitourinary :\n",
    "Infant was circumcised in the newborn nursery which was healing well .\n",
    "Testes were descended bilaterally .\n",
    "Extremities were pink and well perfused .''')\n",
    "\n",
    "# Let's look at a random entity!\n",
    "entity = doc.ents[0]\n",
    "\n",
    "linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "for umls_ent in entity._.kb_ents:\n",
    "\tprint(linker.kb.cui_to_entity[umls_ent[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce3a89247046e31f32bc513870050e3d59f97c892071e52ce4157f905a779d9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
